{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.graph_tokenizer import *\n",
    "from util.process import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GraphTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] : 0\n",
      "[UNK] : 1\n",
      "[CLS] : 2\n",
      "[SEP] : 3\n",
      "[MASK] : 4\n",
      "(0, 'red') : 5\n",
      "(0, 'blue') : 6\n",
      "(0, 'green') : 7\n",
      "(0, 'yellow') : 8\n",
      "(0, 'black') : 9\n",
      "(1, 'red') : 10\n",
      "(1, 'blue') : 11\n",
      "(1, 'green') : 12\n",
      "(1, 'yellow') : 13\n",
      "(1, 'black') : 14\n",
      "(2, 'red') : 15\n",
      "(2, 'blue') : 16\n",
      "(2, 'green') : 17\n",
      "(2, 'yellow') : 18\n",
      "(2, 'black') : 19\n",
      "(3, 'red') : 20\n",
      "(3, 'blue') : 21\n",
      "(3, 'green') : 22\n",
      "(3, 'yellow') : 23\n",
      "(3, 'black') : 24\n",
      "(4, 'red') : 25\n",
      "(4, 'blue') : 26\n",
      "(4, 'green') : 27\n",
      "(4, 'yellow') : 28\n",
      "(4, 'black') : 29\n",
      "('L', 'F', 'T') : 30\n",
      "('L', 'F', 'U') : 31\n",
      "('L', 'F', 'Sz') : 32\n",
      "('L', 'B', 'T') : 33\n",
      "('L', 'B', 'U') : 34\n",
      "('L', 'B', 'Sz') : 35\n",
      "('L', 'Sy', 'T') : 36\n",
      "('L', 'Sy', 'U') : 37\n",
      "('L', 'Sy', 'Sz') : 38\n",
      "('R', 'F', 'T') : 39\n",
      "('R', 'F', 'U') : 40\n",
      "('R', 'F', 'Sz') : 41\n",
      "('R', 'B', 'T') : 42\n",
      "('R', 'B', 'U') : 43\n",
      "('R', 'B', 'Sz') : 44\n",
      "('R', 'Sy', 'T') : 45\n",
      "('R', 'Sy', 'U') : 46\n",
      "('R', 'Sy', 'Sz') : 47\n",
      "('Sx', 'F', 'T') : 48\n",
      "('Sx', 'F', 'U') : 49\n",
      "('Sx', 'F', 'Sz') : 50\n",
      "('Sx', 'B', 'T') : 51\n",
      "('Sx', 'B', 'U') : 52\n",
      "('Sx', 'B', 'Sz') : 53\n",
      "('Sx', 'Sy', 'T') : 54\n",
      "('Sx', 'Sy', 'U') : 55\n",
      "('Sx', 'Sy', 'Sz') : 56\n",
      "0 : [PAD] [PAD] True\n",
      "1 : [UNK] [UNK] True\n",
      "2 : [CLS] [CLS] True\n",
      "3 : [SEP] [SEP] True\n",
      "4 : [MASK] [MASK] True\n",
      "5 : (0, 'red') (0, 'red') True\n",
      "6 : (0, 'blue') (0, 'blue') True\n",
      "7 : (0, 'green') (0, 'green') True\n",
      "8 : (0, 'yellow') (0, 'yellow') True\n",
      "9 : (0, 'black') (0, 'black') True\n",
      "10 : (1, 'red') (1, 'red') True\n",
      "11 : (1, 'blue') (1, 'blue') True\n",
      "12 : (1, 'green') (1, 'green') True\n",
      "13 : (1, 'yellow') (1, 'yellow') True\n",
      "14 : (1, 'black') (1, 'black') True\n",
      "15 : (2, 'red') (2, 'red') True\n",
      "16 : (2, 'blue') (2, 'blue') True\n",
      "17 : (2, 'green') (2, 'green') True\n",
      "18 : (2, 'yellow') (2, 'yellow') True\n",
      "19 : (2, 'black') (2, 'black') True\n",
      "20 : (3, 'red') (3, 'red') True\n",
      "21 : (3, 'blue') (3, 'blue') True\n",
      "22 : (3, 'green') (3, 'green') True\n",
      "23 : (3, 'yellow') (3, 'yellow') True\n",
      "24 : (3, 'black') (3, 'black') True\n",
      "25 : (4, 'red') (4, 'red') True\n",
      "26 : (4, 'blue') (4, 'blue') True\n",
      "27 : (4, 'green') (4, 'green') True\n",
      "28 : (4, 'yellow') (4, 'yellow') True\n",
      "29 : (4, 'black') (4, 'black') True\n",
      "30 : ('L', 'F', 'T') ('L', 'F', 'T') True\n",
      "31 : ('L', 'F', 'U') ('L', 'F', 'U') True\n",
      "32 : ('L', 'F', 'Sz') ('L', 'F', 'Sz') True\n",
      "33 : ('L', 'B', 'T') ('L', 'B', 'T') True\n",
      "34 : ('L', 'B', 'U') ('L', 'B', 'U') True\n",
      "35 : ('L', 'B', 'Sz') ('L', 'B', 'Sz') True\n",
      "36 : ('L', 'Sy', 'T') ('L', 'Sy', 'T') True\n",
      "37 : ('L', 'Sy', 'U') ('L', 'Sy', 'U') True\n",
      "38 : ('L', 'Sy', 'Sz') ('L', 'Sy', 'Sz') True\n",
      "39 : ('R', 'F', 'T') ('R', 'F', 'T') True\n",
      "40 : ('R', 'F', 'U') ('R', 'F', 'U') True\n",
      "41 : ('R', 'F', 'Sz') ('R', 'F', 'Sz') True\n",
      "42 : ('R', 'B', 'T') ('R', 'B', 'T') True\n",
      "43 : ('R', 'B', 'U') ('R', 'B', 'U') True\n",
      "44 : ('R', 'B', 'Sz') ('R', 'B', 'Sz') True\n",
      "45 : ('R', 'Sy', 'T') ('R', 'Sy', 'T') True\n",
      "46 : ('R', 'Sy', 'U') ('R', 'Sy', 'U') True\n",
      "47 : ('R', 'Sy', 'Sz') ('R', 'Sy', 'Sz') True\n",
      "48 : ('Sx', 'F', 'T') ('Sx', 'F', 'T') True\n",
      "49 : ('Sx', 'F', 'U') ('Sx', 'F', 'U') True\n",
      "50 : ('Sx', 'F', 'Sz') ('Sx', 'F', 'Sz') True\n",
      "51 : ('Sx', 'B', 'T') ('Sx', 'B', 'T') True\n",
      "52 : ('Sx', 'B', 'U') ('Sx', 'B', 'U') True\n",
      "53 : ('Sx', 'B', 'Sz') ('Sx', 'B', 'Sz') True\n",
      "54 : ('Sx', 'Sy', 'T') ('Sx', 'Sy', 'T') True\n",
      "55 : ('Sx', 'Sy', 'U') ('Sx', 'Sy', 'U') True\n",
      "56 : ('Sx', 'Sy', 'Sz') ('Sx', 'Sy', 'Sz') True\n"
     ]
    }
   ],
   "source": [
    "# 토큰 생성 후 인코딩 그리고 결과 확인\n",
    "tokens = []\n",
    "test_encode = []\n",
    "for st in tokenizer.special_tokens:\n",
    "    token = st\n",
    "    token_id = tokenizer.encode_token(token)\n",
    "    print(token, ':', token_id)\n",
    "    tokens.append(token)\n",
    "    test_encode.append(token_id)\n",
    "for shape in range(5):\n",
    "    for color in tokenizer.colors:\n",
    "        token = (shape, color)\n",
    "        token_id = tokenizer.encode_token(token)\n",
    "        print(token, ':', token_id)\n",
    "        tokens.append(token)\n",
    "        test_encode.append(token_id)\n",
    "for x_rel in tokenizer.x_rels:\n",
    "    for y_rel in tokenizer.y_rels:\n",
    "        for z_rel in tokenizer.z_rels:\n",
    "            token = (x_rel, y_rel, z_rel)\n",
    "            token_id = tokenizer.encode_token(token)\n",
    "            print(token, ':', token_id)\n",
    "            tokens.append(token)\n",
    "            test_encode.append(token_id)\n",
    "# 생성한거 디코딩 그리고 결과 확인\n",
    "for i in range(len(test_encode)):\n",
    "    token = tokenizer.decode_token(i)\n",
    "    print(i, ':', token, tokens[i], token_compare(token, tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    [((2, 'yellow'), (4, 'green'), ('L', 'Sy', 'T'))],\n",
    "    [((1, 'black'), (2, 'red'), ('R', 'B', 'U'))],\n",
    "    [((3, 'green'), (3, 'red'), ('L', 'Sy', 'U')), ((3, 'red'), (4, 'red'), ('R', 'Sy', 'Sz')), ((4, 'red'), (3, 'green'), ('Sx', 'Sy', 'T'))],\n",
    "    [((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 11, 15, 39, 15, 27, 55, 27, 11, 33, 3, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}\n",
      "{'input_ids': [2, 11, 15, 39, 15, 27, 55, 27, 11, 33, 3, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}\n",
      "{'input_ids': [11, 15, 39, 15, 27, 55, 27, 11, 33, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]}\n",
      "{'input_ids': [11, 15, 39, 15, 27, 55, 27, 11, 33, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}\n",
      "{'input_ids': [2, 11, 15, 39, 15, 27, 55, 27, 11, 33, 3], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [2, 11, 15, 39, 15, 27, 55, 27, 11, 33, 3], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [11, 15, 39, 15, 27, 55, 27, 11, 33], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [11, 15, 39, 15, 27, 55, 27, 11, 33], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] ['[CLS]', (1, 'blue'), (2, 'red'), ('R', 'F', 'T'), (2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U'), (4, 'green'), (1, 'blue'), ('L', 'B', 'T'), '[SEP]', '[PAD]'] [((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] True\n",
      "[((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] ['[CLS]', (1, 'blue'), (2, 'red'), ('R', 'F', 'T'), (2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U'), (4, 'green'), (1, 'blue'), ('L', 'B', 'T'), '[SEP]', '[PAD]', '[PAD]'] [((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] True\n",
      "[((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] [(1, 'blue'), (2, 'red'), ('R', 'F', 'T'), (2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U'), (4, 'green'), (1, 'blue'), ('L', 'B', 'T'), '[PAD]', '[PAD]', '[PAD]'] [((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] True\n",
      "[((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] [(1, 'blue'), (2, 'red'), ('R', 'F', 'T'), (2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U'), (4, 'green'), (1, 'blue'), ('L', 'B', 'T'), '[PAD]', '[PAD]', '[PAD]', '[PAD]'] [((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] True\n",
      "[((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] ['[CLS]', (1, 'blue'), (2, 'red'), ('R', 'F', 'T'), (2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U'), (4, 'green'), (1, 'blue'), ('L', 'B', 'T'), '[SEP]'] [((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] True\n",
      "[((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] ['[CLS]', (1, 'blue'), (2, 'red'), ('R', 'F', 'T'), (2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U'), (4, 'green'), (1, 'blue'), ('L', 'B', 'T'), '[SEP]'] [((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] True\n",
      "[((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] [(1, 'blue'), (2, 'red'), ('R', 'F', 'T'), (2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U'), (4, 'green'), (1, 'blue'), ('L', 'B', 'T')] [((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] True\n",
      "[((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] [(1, 'blue'), (2, 'red'), ('R', 'F', 'T'), (2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U'), (4, 'green'), (1, 'blue'), ('L', 'B', 'T')] [((1, 'blue'), (2, 'red'), ('R', 'F', 'T')), ((2, 'red'), (4, 'green'), ('Sx', 'Sy', 'U')), ((4, 'green'), (1, 'blue'), ('L', 'B', 'T'))] True\n"
     ]
    }
   ],
   "source": [
    "# 인코딩\n",
    "target = test_set[3]\n",
    "\n",
    "print(tokenizer.encode(target, True, 12, True))\n",
    "print(tokenizer.encode(target, True, 13, True))\n",
    "# print(tokenizer.encode(target, True, 3, True))\n",
    "# print(tokenizer.encode(target, True, 1, True))\n",
    "print(tokenizer.encode(target, False, 12, True))\n",
    "print(tokenizer.encode(target, False, 13, True))\n",
    "# print(tokenizer.encode(target, False, 1, True))\n",
    "# print(tokenizer.encode(target, False, 3, True))\n",
    "print(tokenizer.encode(target, True, 12, False))\n",
    "print(tokenizer.encode(target, True, 13, False))\n",
    "# print(tokenizer.encode(target, True, 3, False))\n",
    "# print(tokenizer.encode(target, True, 1, False))\n",
    "print(tokenizer.encode(target, False, 12, False))\n",
    "print(tokenizer.encode(target, False, 13, False))\n",
    "# print(tokenizer.encode(target, False, 3, False))\n",
    "# print(tokenizer.encode(target, False, 1, False))\n",
    "\n",
    "# 디코딩 및 비교\n",
    "encoded = tokenizer.encode(target, True, 12, True)['input_ids']\n",
    "result = graph_compare(target, postprocess_graph(tokenizer.decode(encoded)))\n",
    "print(target, tokenizer.decode(encoded), postprocess_graph(tokenizer.decode(encoded)), result)\n",
    "encoded = tokenizer.encode(target, True, 13, True)['input_ids']\n",
    "result = graph_compare(target, postprocess_graph(tokenizer.decode(encoded)))\n",
    "print(target, tokenizer.decode(encoded), postprocess_graph(tokenizer.decode(encoded)), result)\n",
    "\n",
    "encoded = tokenizer.encode(target, False, 12, True)['input_ids']\n",
    "result = graph_compare(target, postprocess_graph(tokenizer.decode(encoded)))\n",
    "print(target, tokenizer.decode(encoded), postprocess_graph(tokenizer.decode(encoded)), result)\n",
    "encoded = tokenizer.encode(target, False, 13, True)['input_ids']\n",
    "result = graph_compare(target, postprocess_graph(tokenizer.decode(encoded)))\n",
    "print(target, tokenizer.decode(encoded), postprocess_graph(tokenizer.decode(encoded)), result)\n",
    "\n",
    "encoded = tokenizer.encode(target, True, 12, False)['input_ids']\n",
    "result = graph_compare(target, postprocess_graph(tokenizer.decode(encoded)))\n",
    "print(target, tokenizer.decode(encoded), postprocess_graph(tokenizer.decode(encoded)), result)\n",
    "encoded = tokenizer.encode(target, True, 13, False)['input_ids']\n",
    "result = graph_compare(target, postprocess_graph(tokenizer.decode(encoded)))\n",
    "print(target, tokenizer.decode(encoded), postprocess_graph(tokenizer.decode(encoded)), result)\n",
    "\n",
    "encoded = tokenizer.encode(target, False, 12, False)['input_ids']\n",
    "result = graph_compare(target, postprocess_graph(tokenizer.decode(encoded)))\n",
    "print(target, tokenizer.decode(encoded), postprocess_graph(tokenizer.decode(encoded)), result)\n",
    "encoded = tokenizer.encode(target, False, 13, False)['input_ids']\n",
    "result = graph_compare(target, postprocess_graph(tokenizer.decode(encoded)))\n",
    "print(target, tokenizer.decode(encoded), postprocess_graph(tokenizer.decode(encoded)), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2, 18, 27, 36,  3,  0,  0,  0,  0,  0,  0,  0]], dtype=torch.int32), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.int32)}\n",
      "{'input_ids': tensor([[ 2, 18, 27, 36,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 14, 15, 43,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 22, 20, 37, 20, 25, 47, 25, 22, 54,  3,  0,  0],\n",
      "        [ 2, 11, 15, 39, 15, 27, 55, 27, 11, 33,  3,  0,  0]],\n",
      "       dtype=torch.int32), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(test_set[:1], True, 12, True)\n",
    "print(encoded)\n",
    "encoded = tokenizer(test_set, True, 13, True)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 18 27 36 3 0 0 0 0 0 0 0 0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(map(str, encoded['input_ids'][0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
